{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe3IoZ338PDk"
      },
      "source": [
        "#Libs and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB0yRPKCbWya",
        "outputId": "488da5e0-95c8-4c6e-8528-bc2180714c6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gdown\n",
            "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: requests[socks] in c:\\users\\hitch\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\hitch\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gdown) (4.11.1)\n",
            "Requirement already satisfied: six in c:\\users\\hitch\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\hitch\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gdown) (4.64.0)\n",
            "Collecting filelock\n",
            "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hitch\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hitch\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->gdown) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hitch\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->gdown) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hitch\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->gdown) (2022.5.18.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hitch\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Collecting PySocks!=1.5.7,>=1.5.6\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\hitch\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm->gdown) (0.4.5)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (pyproject.toml): started\n",
            "  Building wheel for gdown (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14933 sha256=18db2405e4993e186d78a0618b233b16c83e50f096adab36551d2dba38db4de8\n",
            "  Stored in directory: c:\\users\\hitch\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\38\\72\\83\\dab5cb8321023eb1549d7a7d7f564b479ed4cf71edb70ddad6\n",
            "Successfully built gdown\n",
            "Installing collected packages: PySocks, filelock, gdown\n",
            "Successfully installed PySocks-1.7.1 filelock-3.8.0 gdown-4.5.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script gdown.exe is installed in 'C:\\Users\\hitch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "WARNING: There was an error checking the latest version of pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpSZclcA7rUy",
        "outputId": "e4092ab8-9da4-4efa-c7f2-15186e27c62e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'gdown' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
            "'gdown' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
            "'gdown' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
            "'gdown' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
            "'gdown' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
            "'gdown' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
            "'gdown' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=126dO4VNhLpYKT0TKp18RTAGjrHAl_ZpU -O mnist.npz\n",
        "!gdown https://drive.google.com/uc?id=16wlkaf6GCGX0aJTOtzDo0ypnhqYZ7GVM -O kmnist.npz\n",
        "!gdown https://drive.google.com/uc?id=1XMH39lcD2bnwy4AW3S-4-0Ge7JIYw6CF -O eurosat.npz\n",
        "!gdown https://drive.google.com/uc?id=1BsfU84WJMRRKG3wzRZG6KuCperlLrxHc -O cifar10.npz\n",
        "!gdown https://drive.google.com/uc?id=1MvPjY4m58TW51NZbUIl5tRJbERKMnIk7 -O pathmnist.npz\n",
        "!gdown https://drive.google.com/uc?id=146WDl2VzVdLhnJl5JqYqKVyDPlDQoLDl -O octmnist.npz\n",
        "!gdown https://drive.google.com/uc?id=1BIJFOn5ivB766qNIZdI2Owt8GAmpsmic -O organmnist_axial.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zHwZmGUNbTRf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hitch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import callbacks, optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.applications import InceptionV3, ResNet50V2, EfficientNetB1, DenseNet169\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Lambda, Input\n",
        "from tensorflow.image import resize\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import friedmanchisquare, rankdata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re, os, time, requests\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhvasHsa8VPv"
      },
      "source": [
        "#Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UXE-7P_oKP3G"
      },
      "outputs": [],
      "source": [
        "DATASETS = {\n",
        "\n",
        "    # 'octmnist': { 'shape': (28, 28, 1), 'classes': 4,\n",
        "    #     'phenotypes': [],\n",
        "    # },\n",
        "    'pathmnist': { 'shape': (28, 28, 3), 'classes': 9,\n",
        "        'phenotypes': [],\n",
        "    },\n",
        "    # 'organmnist_axial': { 'shape': (28, 28, 1), 'classes': 11,\n",
        "    #     'phenotypes': [''],\n",
        "    # },\n",
        "    # 'mnist': { 'shape': (28, 28, 1), 'classes': 10,\n",
        "    #     'phenotypes': [],\n",
        "    # },\n",
        "    # 'cifar10': { 'shape': (32, 32, 3), 'classes': 10,\n",
        "    #     'phenotypes': [],\n",
        "    # },   \n",
        "    # 'eurosat': { 'shape': (64, 64, 3), 'classes': 10,\n",
        "    #     'phenotypes': [],\n",
        "    # }, \n",
        "\n",
        "}\n",
        "NUM_TRAINING = 3\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 70"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKmIcKBj8eO-"
      },
      "source": [
        "#Factories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JKOm_msFbalE"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_name):\n",
        "\n",
        "  shape = DATASETS[dataset_name]['shape']\n",
        "  dataset = np.load('%s.npz' % dataset_name, allow_pickle=True)\n",
        "\n",
        "  if dataset_name == 'eurosat':\n",
        "      \n",
        "    print('eurosat')\n",
        "    \n",
        "    train = dataset['train'].tolist()\n",
        "\n",
        "    train_images, train_labels = train['image'], train['label']\n",
        "\n",
        "    train_images = train_images.reshape((train_images.shape[0], *shape))\n",
        "    train_images = train_images.astype(\"float\") / 255.0\n",
        "\n",
        "    train_images, test_images, train_labels, test_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
        "    validation_images, test_images, validation_labels, test_labels = train_test_split(test_images, test_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "  elif dataset_name in ['pathmnist', 'octmnist', 'organmnist_axial']:\n",
        "      \n",
        "    print('medmnist:', dataset_name)\n",
        "    \n",
        "    train_images = dataset['train_images']\n",
        "    validation_images = dataset['val_images']\n",
        "    test_images = dataset['test_images']\n",
        "    train_labels = dataset['train_labels']\n",
        "    validation_labels = dataset['val_labels']\n",
        "    test_labels = dataset['test_labels']\n",
        "\n",
        "    if shape[2] == 1:\n",
        "      train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
        "      validation_images = validation_images.reshape((validation_images.shape[0], 28, 28, 1))\n",
        "      test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
        "\n",
        "    train_images = train_images.astype(\"float\") / 255.0\n",
        "    test_images = test_images.astype(\"float\") / 255.0\n",
        "    validation_images = validation_images.astype(\"float\") / 255.0\n",
        "\n",
        "  else:\n",
        "      \n",
        "    print('outros:', dataset_name)\n",
        "    \n",
        "    train = dataset['train'].tolist()\n",
        "    test = dataset['test'].tolist()\n",
        "\n",
        "    train_images, test_images, train_labels, test_labels = train['image'], test['image'], train['label'], test['label']\n",
        "\n",
        "    train_images = train_images.reshape((train_images.shape[0], *shape))\n",
        "    train_images = train_images.astype(\"float\") / 255.0\n",
        "\n",
        "    test_images = test_images.reshape((test_images.shape[0], *shape))\n",
        "    test_images = test_images.astype(\"float\") / 255.0\n",
        "\n",
        "    validation_images, test_images, validation_labels, test_labels = train_test_split(test_images, test_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "  lb = LabelBinarizer()\n",
        "  train_labels = lb.fit_transform(train_labels)\n",
        "  validation_labels = lb.transform(validation_labels)\n",
        "  test_labels = lb.transform(test_labels)\n",
        "\n",
        "  dataset.close()\n",
        "\n",
        "  return train_images, train_labels, validation_images, validation_labels, test_images, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "41QD3TOGjZiC"
      },
      "outputs": [],
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "  \n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "  precision = true_positives / (predicted_positives + K.epsilon())\n",
        "  recall = true_positives / (possible_positives + K.epsilon())\n",
        "  f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "  return f1_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kQEKzjs78zaA"
      },
      "outputs": [],
      "source": [
        "def build_model(dataset, phenotype):\n",
        "\n",
        "    dataset_shape = DATASETS[dataset]['shape']\n",
        "    dataset_classes = DATASETS[dataset]['classes']\n",
        "\n",
        "    nconv, npool, nfc, nfcneuron = [int(i) for i in re.findall('\\d+', phenotype.split('lr-')[0])]\n",
        "    has_dropout = 'dropout' in phenotype\n",
        "    has_batch_normalization = 'bnorm' in phenotype\n",
        "    has_pool = 'pool' in phenotype\n",
        "    learning_rate = float(phenotype.split('lr-')[1])\n",
        "\n",
        "    # number of filters\n",
        "    filter_size = 32\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=dataset_shape))\n",
        "\n",
        "    # Pooling\n",
        "    for i in range(npool):\n",
        "\n",
        "        # Convolutions\n",
        "        for j in range(nconv):\n",
        "\n",
        "            model.add(layers.Conv2D(filter_size, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "            # Duplicate number of filters for each two convolutions\n",
        "            if (((i + j) % 2) == 1): filter_size = filter_size * 2\n",
        "\n",
        "            # Add batch normalization\n",
        "            if has_batch_normalization:\n",
        "                model.add(layers.BatchNormalization())\n",
        "\n",
        "        # Add pooling\n",
        "        if has_pool:\n",
        "            model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "            # Add dropout\n",
        "            if has_dropout:\n",
        "                model.add(layers.Dropout(0.25))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # fully connected\n",
        "    for i in range(nfc):\n",
        "        model.add(layers.Dense(nfcneuron))\n",
        "        model.add(layers.Activation('relu'))\n",
        "\n",
        "    if has_dropout:\n",
        "        model.add(layers.Dropout(0.5))\n",
        "\n",
        "    model.add(layers.Dense(dataset_classes, activation='softmax'))\n",
        "\n",
        "    opt = optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    return model, opt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-KjZKEH8j3a"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O6eGNXPReUoX"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataset):\n",
        "\n",
        "  train_images, train_labels, validation_images, \\\n",
        "    validation_labels, test_images, test_labels = load_dataset(dataset)\n",
        "\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(BATCH_SIZE, drop_remainder=True)\n",
        "  validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels)).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  accuracies, f1_scores = [], []\n",
        "\n",
        "  for i in range(NUM_TRAINING):\n",
        "\n",
        "    print('Training %s of %s' % (i + 1, NUM_TRAINING))\n",
        "\n",
        "    history = model.fit(train_ds,\n",
        "            epochs=EPOCHS, \n",
        "            validation_data=validation_ds,\n",
        "            verbose=1)\n",
        "\n",
        "    loss, accuracy, f1_score = model.evaluate(test_images, test_labels, verbose=1)\n",
        "\n",
        "    print(accuracy, f1_score)\n",
        "\n",
        "    accuracies.append(accuracy)\n",
        "    f1_scores.append(f1_score)\n",
        "\n",
        "  return {\n",
        "      'accuracy': np.mean(accuracies),\n",
        "      'accuracy_sd': np.std(accuracies),\n",
        "      'f1_score': np.mean(f1_scores),\n",
        "      'f1_score_sd': np.std(f1_scores),\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnEp86OdlYRX",
        "outputId": "f9a46c05-bfbe-4aef-e536-c69f17455b93"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Please provide a TPU Name to connect to.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\laragon\\www\\mestrado\\python\\train-cnn-without-api.ipynb Célula: 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/laragon/www/mestrado/python/train-cnn-without-api.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tpu \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mcluster_resolver\u001b[39m.\u001b[39;49mTPUClusterResolver\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/laragon/www/mestrado/python/train-cnn-without-api.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tpu_strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTPUStrategy(tpu)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\distribute\\cluster_resolver\\tpu\\tpu_cluster_resolver.py:106\u001b[0m, in \u001b[0;36mTPUClusterResolver.connect\u001b[1;34m(tpu, zone, project)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(tpu\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             zone\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     74\u001b[0m             project\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     75\u001b[0m   \u001b[39m\"\"\"Initializes TPU and returns a TPUClusterResolver.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \n\u001b[0;32m     77\u001b[0m \u001b[39m  This API will connect to remote TPU cluster and initialize the TPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    NotFoundError: If no TPU devices found in eager mode.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m   resolver \u001b[39m=\u001b[39m TPUClusterResolver(tpu, zone, project)\n\u001b[0;32m    107\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m remote  \u001b[39m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m    108\u001b[0m   remote\u001b[39m.\u001b[39mconnect_to_cluster(resolver)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\distribute\\cluster_resolver\\tpu\\tpu_cluster_resolver.py:198\u001b[0m, in \u001b[0;36mTPUClusterResolver.__init__\u001b[1;34m(self, tpu, zone, project, job_name, coordinator_name, coordinator_address, credentials, service, discovery_url)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\"\"\"Creates a new TPUClusterResolver object.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[39mThe ClusterResolver will then use the parameters to query the Cloud TPU APIs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[39m    Google Cloud environment.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mif\u001b[39;00m tpu \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    197\u001b[0m   \u001b[39m# Default Cloud environment\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cloud_tpu_client \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mClient(\n\u001b[0;32m    199\u001b[0m       tpu\u001b[39m=\u001b[39;49mtpu,\n\u001b[0;32m    200\u001b[0m       zone\u001b[39m=\u001b[39;49mzone,\n\u001b[0;32m    201\u001b[0m       project\u001b[39m=\u001b[39;49mproject,\n\u001b[0;32m    202\u001b[0m       credentials\u001b[39m=\u001b[39;49mcredentials,\n\u001b[0;32m    203\u001b[0m       service\u001b[39m=\u001b[39;49mservice,\n\u001b[0;32m    204\u001b[0m       discovery_url\u001b[39m=\u001b[39;49mdiscovery_url)\n\u001b[0;32m    205\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tpu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cloud_tpu_client\u001b[39m.\u001b[39mname()\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39m# Directly connected TPU environment\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\tpu\\client\\client.py:142\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, tpu, zone, project, credentials, service, discovery_url)\u001b[0m\n\u001b[0;32m    139\u001b[0m tpu \u001b[39m=\u001b[39m _get_tpu_name(tpu)\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m tpu \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mPlease provide a TPU Name to connect to.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tpu \u001b[39m=\u001b[39m _as_text(tpu)\n\u001b[0;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_api \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tpu\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mgrpc://\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: Please provide a TPU Name to connect to."
          ]
        }
      ],
      "source": [
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JwD-VuR3KuI",
        "outputId": "6bd19ff5-431f-422f-fd19-e85f4b09a230",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATASET: cifar10\n",
            "PHENOTYPE: (((conv*3)bnorm-pool-dropout)*3)fc*1*256*lr-0.001\n",
            "Building model.\n",
            "Model created.\n",
            "Begining training...\n",
            "outros: cifar10\n",
            "Training 1 of 3\n",
            "Epoch 1/70\n",
            "390/390 [==============================] - 95s 236ms/step - loss: 1.8175 - accuracy: 0.3691 - f1_score: 0.2407 - val_loss: 2.4535 - val_accuracy: 0.2563 - val_f1_score: 0.1809\n",
            "Epoch 2/70\n",
            "390/390 [==============================] - 94s 238ms/step - loss: 1.2548 - accuracy: 0.5608 - f1_score: 0.5071 - val_loss: 1.0007 - val_accuracy: 0.6502 - val_f1_score: 0.6309\n",
            "Epoch 3/70\n",
            "390/390 [==============================] - 94s 238ms/step - loss: 1.0252 - accuracy: 0.6505 - f1_score: 0.6225 - val_loss: 0.8926 - val_accuracy: 0.6859 - val_f1_score: 0.6752\n",
            "Epoch 4/70\n",
            "390/390 [==============================] - 94s 238ms/step - loss: 0.8782 - accuracy: 0.7011 - f1_score: 0.6892 - val_loss: 0.8703 - val_accuracy: 0.7045 - val_f1_score: 0.6947\n",
            "Epoch 5/70\n",
            "390/390 [==============================] - 94s 239ms/step - loss: 0.8753 - accuracy: 0.7126 - f1_score: 0.7024 - val_loss: 1.3561 - val_accuracy: 0.5500 - val_f1_score: 0.5618\n",
            "Epoch 6/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.7983 - accuracy: 0.7385 - f1_score: 0.7290 - val_loss: 0.6632 - val_accuracy: 0.7753 - val_f1_score: 0.7750\n",
            "Epoch 7/70\n",
            "390/390 [==============================] - 94s 238ms/step - loss: 0.6410 - accuracy: 0.7901 - f1_score: 0.7836 - val_loss: 0.6419 - val_accuracy: 0.7859 - val_f1_score: 0.7860\n",
            "Epoch 8/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.6340 - accuracy: 0.7910 - f1_score: 0.7894 - val_loss: 0.7310 - val_accuracy: 0.7717 - val_f1_score: 0.7673\n",
            "Epoch 9/70\n",
            "390/390 [==============================] - 93s 238ms/step - loss: 0.5899 - accuracy: 0.8057 - f1_score: 0.8033 - val_loss: 0.5598 - val_accuracy: 0.8112 - val_f1_score: 0.8150\n",
            "Epoch 10/70\n",
            "390/390 [==============================] - 93s 237ms/step - loss: 0.5626 - accuracy: 0.8125 - f1_score: 0.8133 - val_loss: 1.2955 - val_accuracy: 0.6042 - val_f1_score: 0.6021\n",
            "Epoch 11/70\n",
            "390/390 [==============================] - 94s 239ms/step - loss: 0.5046 - accuracy: 0.8320 - f1_score: 0.8322 - val_loss: 0.5601 - val_accuracy: 0.8243 - val_f1_score: 0.8262\n",
            "Epoch 12/70\n",
            "390/390 [==============================] - 94s 239ms/step - loss: 0.3888 - accuracy: 0.8690 - f1_score: 0.8695 - val_loss: 0.5648 - val_accuracy: 0.8346 - val_f1_score: 0.8411\n",
            "Epoch 13/70\n",
            "390/390 [==============================] - 94s 239ms/step - loss: 0.3382 - accuracy: 0.8869 - f1_score: 0.8871 - val_loss: 0.5685 - val_accuracy: 0.8359 - val_f1_score: 0.8388\n",
            "Epoch 14/70\n",
            "390/390 [==============================] - 94s 239ms/step - loss: 0.3040 - accuracy: 0.8974 - f1_score: 0.8981 - val_loss: 0.5583 - val_accuracy: 0.8390 - val_f1_score: 0.8456\n",
            "Epoch 15/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.2748 - accuracy: 0.9062 - f1_score: 0.9061 - val_loss: 0.5578 - val_accuracy: 0.8473 - val_f1_score: 0.8493\n",
            "Epoch 16/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.2525 - accuracy: 0.9142 - f1_score: 0.9152 - val_loss: 0.5881 - val_accuracy: 0.8391 - val_f1_score: 0.8442\n",
            "Epoch 17/70\n",
            "390/390 [==============================] - 94s 238ms/step - loss: 0.2220 - accuracy: 0.9242 - f1_score: 0.9254 - val_loss: 0.5901 - val_accuracy: 0.8414 - val_f1_score: 0.8444\n",
            "Epoch 18/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.2120 - accuracy: 0.9292 - f1_score: 0.9291 - val_loss: 0.5845 - val_accuracy: 0.8459 - val_f1_score: 0.8506\n",
            "Epoch 19/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.2014 - accuracy: 0.9320 - f1_score: 0.9321 - val_loss: 0.6389 - val_accuracy: 0.8407 - val_f1_score: 0.8452\n",
            "Epoch 20/70\n",
            "390/390 [==============================] - 94s 239ms/step - loss: 0.1838 - accuracy: 0.9377 - f1_score: 0.9382 - val_loss: 0.6278 - val_accuracy: 0.8550 - val_f1_score: 0.8578\n",
            "Epoch 21/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.1578 - accuracy: 0.9466 - f1_score: 0.9466 - val_loss: 0.6719 - val_accuracy: 0.8535 - val_f1_score: 0.8562\n",
            "Epoch 22/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.1675 - accuracy: 0.9458 - f1_score: 0.9463 - val_loss: 0.6519 - val_accuracy: 0.8502 - val_f1_score: 0.8513\n",
            "Epoch 23/70\n",
            "390/390 [==============================] - 95s 241ms/step - loss: 0.1478 - accuracy: 0.9513 - f1_score: 0.9520 - val_loss: 0.7185 - val_accuracy: 0.8489 - val_f1_score: 0.8507\n",
            "Epoch 24/70\n",
            "390/390 [==============================] - 95s 241ms/step - loss: 0.1315 - accuracy: 0.9564 - f1_score: 0.9570 - val_loss: 0.6637 - val_accuracy: 0.8531 - val_f1_score: 0.8559\n",
            "Epoch 25/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.1300 - accuracy: 0.9576 - f1_score: 0.9574 - val_loss: 0.7184 - val_accuracy: 0.8487 - val_f1_score: 0.8516\n",
            "Epoch 26/70\n",
            "390/390 [==============================] - 95s 242ms/step - loss: 0.1258 - accuracy: 0.9601 - f1_score: 0.9601 - val_loss: 0.7385 - val_accuracy: 0.8463 - val_f1_score: 0.8481\n",
            "Epoch 27/70\n",
            "390/390 [==============================] - 95s 241ms/step - loss: 0.1156 - accuracy: 0.9627 - f1_score: 0.9625 - val_loss: 0.7679 - val_accuracy: 0.8435 - val_f1_score: 0.8472\n",
            "Epoch 28/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.1111 - accuracy: 0.9634 - f1_score: 0.9636 - val_loss: 0.6713 - val_accuracy: 0.8657 - val_f1_score: 0.8677\n",
            "Epoch 29/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.1006 - accuracy: 0.9683 - f1_score: 0.9685 - val_loss: 0.7802 - val_accuracy: 0.8601 - val_f1_score: 0.8608\n",
            "Epoch 30/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.1046 - accuracy: 0.9666 - f1_score: 0.9667 - val_loss: 0.7449 - val_accuracy: 0.8582 - val_f1_score: 0.8599\n",
            "Epoch 31/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.0976 - accuracy: 0.9688 - f1_score: 0.9689 - val_loss: 0.8398 - val_accuracy: 0.8451 - val_f1_score: 0.8485\n",
            "Epoch 32/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.1026 - accuracy: 0.9686 - f1_score: 0.9684 - val_loss: 0.7449 - val_accuracy: 0.8575 - val_f1_score: 0.8605\n",
            "Epoch 33/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.0854 - accuracy: 0.9729 - f1_score: 0.9733 - val_loss: 0.8090 - val_accuracy: 0.8572 - val_f1_score: 0.8590\n",
            "Epoch 34/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.0821 - accuracy: 0.9738 - f1_score: 0.9740 - val_loss: 0.7570 - val_accuracy: 0.8459 - val_f1_score: 0.8496\n",
            "Epoch 35/70\n",
            "390/390 [==============================] - 95s 241ms/step - loss: 0.0931 - accuracy: 0.9714 - f1_score: 0.9714 - val_loss: 0.8072 - val_accuracy: 0.8514 - val_f1_score: 0.8551\n",
            "Epoch 36/70\n",
            "390/390 [==============================] - 94s 239ms/step - loss: 0.0775 - accuracy: 0.9751 - f1_score: 0.9752 - val_loss: 0.9392 - val_accuracy: 0.8499 - val_f1_score: 0.8513\n",
            "Epoch 37/70\n",
            "390/390 [==============================] - 94s 240ms/step - loss: 0.0740 - accuracy: 0.9766 - f1_score: 0.9767 - val_loss: 0.7325 - val_accuracy: 0.8682 - val_f1_score: 0.8692\n",
            "Epoch 38/70\n",
            "390/390 [==============================] - 94s 239ms/step - loss: 0.0715 - accuracy: 0.9780 - f1_score: 0.9780 - val_loss: 0.7517 - val_accuracy: 0.8658 - val_f1_score: 0.8675\n",
            "Epoch 39/70\n",
            "239/390 [=================>............] - ETA: 35s - loss: 0.0690 - accuracy: 0.9783 - f1_score: 0.9780"
          ]
        }
      ],
      "source": [
        "for dataset in DATASETS:\n",
        "\n",
        "  for phenotype in DATASETS[dataset]['phenotypes']:\n",
        "\n",
        "    print('DATASET:', dataset)\n",
        "    print('PHENOTYPE:', phenotype)\n",
        "\n",
        "    print('Building model.')\n",
        "\n",
        "    model, opt = build_model(dataset, phenotype)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', f1_score])\n",
        "    # model.summary()\n",
        "\n",
        "    print('Model created.')\n",
        "\n",
        "    print('Begining training...')\n",
        "\n",
        "    fitness = train_model(model, dataset)\n",
        "\n",
        "    print('FITNESS:', fitness)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "analise_amerson.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "561ba7334891d5557048400018ed70314ab15183eccc04b7a729dbd16cb89d94"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
